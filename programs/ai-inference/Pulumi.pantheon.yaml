encryptionsalt: v1:5dZ+M8HAl+M=:v1:hmFWjBex0KQxsx57:5PsrriLPriqOzC3ApfIRuGPTEuYghQ==
config:
  kubernetes:clusterIdentifier: pantheon
  kubernetes:context: pantheon
  ai-inference:namespace: "ai-inference"
  ai-inference:defaults:
    modelCache:
      size: "200Gi"
      nfs:
        server: "mars.holdenitdown.net"
        path: "/export/models"
        readOnly: false
    resources:
      requests:
        memory: "32Gi"
        cpu: "8000m"
      limits:
        memory: "128Gi"
        cpu: "24000m"
    tolerations:
      - key: "workload-type"
        operator: "Equal"
        value: "gpu-inference"
        effect: "NoSchedule"
    nodeSelector:
      rholden.dev/gpu: "cuda"
  ai-inference:models:
    # - model:
    #     name: "zai-org/GLM-4.6V-Flash"
    #     dtype: "bfloat16"
    #     maxModelLen: 32768
    #   image: "cr.holdenitdown.net/rfhold/vllm:arm64-v0.13.0-tf5"
    #   runtimeClassName: "nvidia"
    #   inference:
    #     tensorParallelSize: 1
    #     gpuMemoryUtilization: 0.80
    #     maxNumSeqs: 64
    #     enableChunkedPrefill: true
    #     swapSpace: 4
    #     enableAutoToolChoice: true
    #     toolCallParser: "glm45"
    #   replicas: 1
    #   weight: 100
    #   ingress:
    #     enabled: true
    #     className: "internal"
    #     host: "glm-4v.holdenitdown.net"
    #     annotations:
    #       cert-manager.io/cluster-issuer: "letsencrypt-prod"
    #     tls:
    #       enabled: true
    #       secretName: "glm-4v-tls"
    - model:
        name: "Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8"
        dtype: "auto"
        maxModelLen: 86000
      image: "cr.holdenitdown.net/rfhold/vllm:arm64-v0.13.0-tf5"
      runtimeClassName: "nvidia"
      inference:
        tensorParallelSize: 1
        gpuMemoryUtilization: 0.85
        maxNumSeqs: 256
        enableChunkedPrefill: true
        swapSpace: 4
        enableExpertParallel: true
        enableAutoToolChoice: true
        toolCallParser: "qwen3_coder"
      replicas: 1
      weight: 100
      ingress:
        enabled: true
        className: "internal"
        host: "qwen3-coder.holdenitdown.net"
        annotations:
          cert-manager.io/cluster-issuer: "letsencrypt-prod"
        tls:
          enabled: true
          secretName: "qwen3-coder-tls"
    - model:
        name: "zai-org/GLM-4.7-Flash"
        dtype: "half"
        maxModelLen: 65536
      inference:
        tensorParallelSize: 1
        gpuMemoryUtilization: 0.85
        maxNumSeqs: 64
        swapSpace: 10
        enableChunkedPrefill: true
        enableAutoToolChoice: true
        toolCallParser: "glm47"
        reasoningParser: "glm45"
        enforceEager: true
      replicas: 1
      weight: 100
      image: "cr.holdenitdown.net/rfhold/vllm:kyuz0-main"
      imagePullPolicy: "Always"
      nodeSelector:
        rholden.dev/gpu: "gfx1151"
      resources:
        requests:
          memory: "12Gi"
          cpu: "4000m"
        limits:
          memory: "64Gi"
          cpu: "16000m"
      env:
        VLLM_TARGET_DEVICE: "rocm"
        PYTORCH_ROCM_ARCH: "gfx1151"
        TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL: "1"
        ROCBLAS_USE_HIPBLASLT: "1"
        FLASH_ATTENTION_TRITON_AMD_ENABLE: "TRUE"
        HIP_FORCE_DEV_KERNARG: "1"
      hostDevices:
        - "/dev/kfd"
        - "/dev/dri"
      hostIPC: true
      podSecurityContext:
        supplementalGroups:
          - 992
      securityContext:
        privileged: true
      ingress:
        enabled: true
        className: "internal"
        host: "vllm.holdenitdown.net"
        annotations:
          cert-manager.io/cluster-issuer: "letsencrypt-prod"
        tls:
          enabled: true
          secretName: "vllm-tls"
  ai-inference:sharedPool:
    hostname: "inference.holdenitdown.net"
    createHttpRoute: true
    clusterIssuer: "letsencrypt-prod"
    tlsSecretName: "ai-inference-gateway-tls"
